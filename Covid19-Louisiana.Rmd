---
title: "COVID-19 in Louisiana - A Time Series Analysis"
author: "Dustin Bracy"
date: "12/5/2020"
output: 
  html_document:
    code_folding: hide
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
options(scipen = 999)

library(tswge)
library(tidyverse)
library(ggthemes)
library(RColorBrewer)
library(gridExtra)
library(vars)
library(nnfor)
#theme_set(theme_fivethirtyeight())

```

# Introduction

My name is Dustin Bracy, and this is a time series analysis of COVID-19 data in Louisiana and the United States spanning an 8 month period from the declaration of the pandemic on March 11, 2020 through November 11, 2020.  Below I will explore the difference between new daily positive cases and positivity rates, and why one may be more advantageous.  Next I'll walk through univiariate and multivariate analyses of both Louisiana and national numbers.  Finally I'll wrap up with a comparison of model performance for each use case.

# Acquire and Load the Data

The COVID-19 data used here comes from two primary sources: the [Louisiana Department of Health Coronavirus](https://ldh.la.gov/Coronavirus/) website, and  [The Covid Tracking Project](https://covidtracking.com/) website.  Additional temperature, windspeed and other related data for Louisiana was collected manually from historical data for the MSY (New Orleans regional airport) section of  [Weather Underground](https://www.wunderground.com/history/monthly/us/la/kenner/KMSY/date/2020-11).  

The data comes in pretty good shape, with only a handful of missing values in the US dataset.  Fortunately, there are no missing values in response variables of interest (i.e. the total number of daily tests and the number of daily positive cases).

The data ingestion pipeline is fairly simple: download the data directly from the api or website, remove data outside of the March 11 to November 11 window, and add positivity metrics. 

```{r get_Data}

# Get latest US Data & sort by date ascending
download.file('https://covidtracking.com/data/download/national-history.csv',
              destfile = './data/national-history.csv')

us <- read_csv('./data/national-history.csv')
us <- us[order(us$date),]


# Get latest LA Data & consolidate to state level
download.file('https://ldh.la.gov/assets/oph/Coronavirus/data/LA_COVID_TESTBYDAY_PARISH_PUBLICUSE.xlsx',
              destfile = './data/LA_COVID_TESTBYDAY_PARISH_PUBLICUSE.xlsx')

la <- readxl::read_xlsx('./data/LA_COVID_TESTBYDAY_PARISH_PUBLICUSE.xlsx')
la <- la %>%
  group_by(date = `Lab Collection Date`) %>%
  summarise(
    tests = sum(`Daily Test Count`),
    positive = sum(`Daily Positive Test Count`),
    negative = sum(`Daily Negative Test Count`),
    cases = sum(`Daily Case Count`)
  )


# Create positivity features
la$positivity = la$positive / la$tests
us$positivity <- us$positiveIncrease / us$totalTestResultsIncrease


# Get temp data from WeatherUnderground.com (historical data for MSY)
## Note Nov8 data missing from site, so the imputed average of Nov7+9 was taken
xdf <- read_csv('./data/temp.csv')
xdf$GatheringBinary <- as.numeric(ifelse(is.na(xdf$Gathering), 0,1))

# Trim all datasets to span March 11, 2020 through Nov 11, 2020
la <- la[11:256,]
us <- us[50:295,]
xdf <- xdf[11:256,]

# Check for missing values
la %>% is.na() %>% summary()
us %>% is.na() %>% summary()
xdf %>% dplyr::select(!Gathering) %>% is.na() %>% summary()

```

# Comparing Positivity Rate vs New Positive Cases

*Positivity rate* - According to John Hopkins University, the positivity rate is a new measure developed to identify either trending increases in transmission rates, or overall drops in testing rates, which can indicate a shortage of tests.  It measures the ratio of diagnostic tests which are actually positive (i.e. # of positive tests / # of total tests). 

The idea behind this test is that a high positivity score should give leaders indication that either infection is increasing at an unusual rate, or not enough tests are available.  The World Health Organization has recommended nations adopt a rule of thumb to keep populations under approximately 5% positivity rate before considering re-opening or relaxing social restrictions.

The positivity rate improves upon strictly measuring the total number of cases or positive results by better accounting for new tests. As time goes on, more individuals will be re-tested, including high risk individuals such as health care workers, who may receive tests and positive results more frequently than others.  The positivity rate accounts for this by measuring the total number of positive cases against tests, which will show less bias than strictly reporting on positive cases, which can be inflated by duplicate tests and non-unique cases.  A better way to control this bias is to measure the people testing positive over the people who have been tested, although this data is much more difficult to collect, due to HIPAA and privacy concerns with linking data to individuals.

*Sources*
https://www.jhsph.edu/covid-19/articles/covid-19-testing-understanding-the-percent-positive.html
https://www.mprnews.org/story/2020/08/12/behind-the-numbers-what-does-a-covid19-positivity-rate-really-mean

Below is a plot that shows the differences between new positive cases vs the positivity rate.  You can start to see the explosive nature of the positive case metric.  I determined the ratio of Louisiana's population compared to the US population by using census.gov population values from Jul 1, 2019.

```{r PositivityComparison}
#census.gov reported populations on Jul 1, 2019: Louisiana / USA
lapopratio <- 4648794 / 328239523

positivity_df <- data.frame(cbind(la[c('date', 'positivity')],  us['positivity']))
names(positivity_df) = c('date','LA','US')

positive_df <- data.frame(cbind(la[c('date', 'positive')],  us['positiveIncrease']))
names(positive_df) = c('date','LA','US')
positive_df$US = positive_df$US * lapopratio


grid.arrange(
  gather(positivity_df, key = Region, value = 'Positivity Rate', -date) %>% 
    ggplot(aes(date, `Positivity Rate`)) + 
    geom_line(aes(color=Region), size=1) + 
    labs(title='Louisiana vs US Positivity Rate', 
         x = 'Date', 
         y='Positivity Rate') +
    theme_fivethirtyeight(), 

  gather(positive_df, key = Region, value = 'Positive Tests', -date) %>% 
    ggplot(aes(date, `Positive Tests`)) + 
    geom_line(aes(color=Region), size=1) + 
    labs(title='Louisiana vs US Daily New Positive Tests', 
         x = 'Date', 
         y='Positive Tests', 
         subtitle = '*US numbers scaled to 1.42% based on LA population density'),
  ncol = 2)

```

# Evaluation Methods

To compare the prediction performance of our models to reality, we want to train a model and hold back a test set of a given period of days that I'll call a  *horizon*.  We will sum the squared difference between each predicted value and actual value, and then take the average to generate the Average Squared Error (ASE).  This metric will be the base comparison for all models, and generally the model with the lowest ASE score is the best performing model.

## Helper functions

To help score the models, I have written a couple of helper functions.  A takes a fitted model, predictions and the desired horizon, and it generates plots of actual, predicted and confidence intervals and the ASE for the given model.

```{r eval_model_function}

eval_model <- function(response, predictions, pred_ul = NA, pred_ll = NA, model_name, AIC_val = 0, ending_point = length(response)) {
  num_predictions = length(predictions)
  test_stop <- length(response)
  test_start <- test_stop - num_predictions + 1
  compare_stop <- test_start - 1
  compare_start <- compare_stop - num_predictions + 1
  ASE <- mean((predictions - response[test_start:test_stop])^2)

  # Build predictions dataframe
  df <- data.frame('Predicted' = predictions)
  df$Day = row(df)
  df$Actual = response[test_start:test_stop]
  df <- gather(df, key='Type', value, -Day)
  
  #if we have enough data to plot num_predictions * 2, do it, else use num_predictions
  starting_point <- compare_start - num_predictions + 1
  plot_start <- ifelse(starting_point < 0, compare_start, starting_point)
  day_multiplier <- ifelse(starting_point < 0, -1,-2)
  
  # Build predicted vs actual dataframe
  df <- rbind(df, 
    data.frame("Day"=c(((num_predictions-1)*day_multiplier):0), 
               "Type" = 'Actual', 
               value = response[plot_start:compare_stop]))

  # Built UL/LL dataframes
  ul <- data.frame("Day"=c(1:num_predictions), pred_ul)
  ll <- data.frame("Day"=c(1:num_predictions), pred_ll)
  
  # Build Plot
  comparison_plot <- ggplot() + 
    geom_line(data=df, aes(Day + ending_point - num_predictions, value, color=Type)) + 
    geom_point(size=.75) + 
    labs(title=paste(model_name, 'Performance Evaluation'),
         subtitle=paste0(num_predictions,'-Day Forecast'), 
         x='Day', 
         y='Positivity Rate',
         caption=paste0('ASE: ',round(ASE,6),
                       '\nAIC: ',round(AIC_val,6)))
  
  # Add confidence intervals if supplied
  if (length(pred_ul) == length(predictions)){
    comparison_plot = comparison_plot + 
      geom_line(aes(ul$Day + ending_point - num_predictions, ul$pred_ul), 
                color='grey70', linetype = "dashed") 
  }
  if (length(pred_ll) == length(predictions)){
    comparison_plot = comparison_plot + 
      geom_line(aes(ll$Day + ending_point - num_predictions, ll$pred_ll), 
                color='grey70', linetype = "dashed") 
  }
  
  return(comparison_plot)
}

```

```{r eval_model_example, eval=FALSE}
########## example: ARIMA(12,2) ########## 
e <- est.arma.wge(us$positivity, p=12, q=2)
preds <- fore.aruma.wge(us$positivity, phi = e$phi, theta=e$theta,  n.ahead = 7, lastn = T, limits = F)
eval_model(us$positivity,preds$f, preds$ul, preds$ll, 'ARMA(12,2)', e$aic) 

preds <- fore.aruma.wge(us$positivity, phi = e$phi, theta=e$theta,  n.ahead = 90, lastn = T, limits = F)
eval_model(us$positivity,preds$f, preds$ul, preds$ll, 'ARMA(12,2)', e$aic) 

```

Another function calculates the rolling ASE, which is a measure of the ASE across several windows in time, for a fitted model. This function uses the evaluation function to plot each window and capture score, and then stores them in a list.  This list is then averaged to find the average ASE across several time periods.  By default I use a month of training data to evaluate a seven day period.  The function finally returns a plot of all windows and actual values to visualize performance across the range of data.

```{r Rolling_ASE_function}

rolling_ASE <- function (df, fitted_model, d=0, s=0, horizon=7, training_size=30, model_name, model_type = 'ARUMA', maxP){
  ASE = list(ASE = c(), plots = c(), multiplot = NA)
  ASE_holder = c()
  comp_df <- df %>% dplyr::select(date, positivity)
  comp_df$preds = NA
  names(comp_df) = c('date','Actual','Predicted')
  test_stop <- length(df$positivity)
  loop_end <- floor(test_stop/(training_size+horizon))
  
  for (x in 1:loop_end){
    test_start <- test_stop - horizon + 1
    train_start <- test_start - training_size 
    train_stop <- test_start - 1
    print(paste0('test window: ',test_start,':',test_stop,
                 ', train window: ',train_start,':',train_stop))
    data_window <- df$positivity[train_start:train_stop]
    
    if(model_type == 'ARUMA') {
      preds <- fore.aruma.wge(data_window, 
                              phi=fitted_model$phi, 
                              theta=fitted_model$theta, 
                              s=s, 
                              d=d, 
                              n.ahead = horizon, 
                              lastn = F, 
                              limits = F)
      pred_object <- preds$f
    }
    if(model_type == 'SigPlusNoise') {
      preds <- fore.sigplusnoise.wge(data_window, max.p = maxP, n.ahead = horizon, limits=F)
      pred_object <- preds$f
    }
    
    if(model_type == 'NNFOR') {
      ts_la <- ts(data_window, start = '1')
      mlp_model = mlp(ts_la, lags = horizon, hd.auto.type = 'cv', reps=100)
      preds <- predict(mlp_model, horizon)
      preds$ul <- NA
      preds$ll <- NA
      pred_object <- as.numeric(preds$mean)
    }
    a <- mean((pred_object - df$positivity[test_start:test_stop])^2)
    print(a)
    print(class(a))
    print(pred_object)
    #ASE_holder[x] <- a
    ASE$ASE[x] <- a
    comp_df$Predicted[test_start:test_stop] = pred_object

    ASE$plots[x] <-
      plot(eval_model(
        data_window,
        pred_object, 
        model_name = model_name, 
        AIC_val = fitted_model$aic, 
        pred_ul = preds$ul, 
        pred_ll = preds$ll,
        ending_point = test_stop))
    test_stop = test_stop - training_size
    
  }
  
  ASE$multiplot <- plot(gather(comp_df, key = Type, value = 'Positivity_Rate', -date) %>% 
         ggplot(aes(date, Positivity_Rate, color=Type)) + geom_line() +
         labs(
           title=paste(model_name, 'Performance Evaluation'),
           subtitle=paste0(horizon,'-Day Forecast Rolling Window'), 
           x='Day', 
           y='Positivity Rate',
           caption=paste0('Mean ASE: ',round(mean(ASE$ASE),6))
           #caption=paste0('Mean ASE: ',round(mean(ASE_holder),6))
         )
       ) 
  return(ASE)
}

```

```{r Rolling_ASE_example, eval=FALSE}
########## example: ARIMA(12,2) ########## 
e <- est.arma.wge(us$positivity, p=12, q=2)
test <- rolling_ASE(us, e, s=12, d=1, horizon=7, model_name = 'ARMA(12,2)')

```

# Univariate Analysis

The steps for building and evaluating our models will proceed as follows:
- Plot and examine the data
  - Realization
  - Spectral Density
  - Autocorrelation Function
- Evaluate stationarity assumptions. There are three stationary conditions:
  - Mean does not depend on time
  - Variance is finite and does not depend on time
  - Correlation only depends on how far apart observations are in time, not where they are
- Build models
  - Check that serial correlation has been removed (AIC scores, ACF, PACF plots)
  - Check residuals for white noise
- Evaluate model forecast performance

## Louisiana

### Plot the Data:

```{r LA_plots}
# Plot the data
plotts.sample.wge(la$positivity)
# Look deeper at spectral density
parzen.wge(la$positivity, trun=100)
```

There is strong evidence of serial correlation in this data as can be seen in the peaks in the spectral density plots and the slowly damping, sinusoidal behavior in the autocorrelation function plots.  There is significant evidence of a weekly trend, as can be seen in the Parzen Window (spectral density plot).  The frequency peak at .14 indicates a weekly trend (1/7 = .143), with potentially an echo at .28.  Increasing the truncation point of the Parzen function helps highlight some additional points that suggest possibly a monthly factor as well.  

### Evaluate Stationarity:
There are several indications that suggest the data is NOT stationary, the most obvious being the strong weekly trend with strong peaks each weekend, which effectively increases the mean on weekends.  There is a difference in variance which can be seen in the early weeks of the pandemic that have tapered off as time progresses. 

### Model Construction:
I start by differencing the data, which filters long-term trending behavior and leaves cyclical patterns. Differencing once leaves a good bit of cyclical pattern in the ACF, especially at lag 7.

#### Identify correlation structures:
Here we are going to look at different filters to try to model out some of the correlation in the data.  We'll also look at a factor table to see if we can identify a weekly or monthly trend.

The overfit table shows strong evidence of 1-B, 1+.445B+B^2, 1-1.247B+B^2 and 1+.802B+B^2 being present in the model.  This is more than enough evidence to proceed with the weekly seasonal model.  
```{r Louisiana_correlation}
# Take out 1-B
la.d1 <- artrans.wge(la$positivity,1)

# Taking out another 1-B.. looks like it isn't enough
artrans.wge(la.d1,1)

# strong evidence of a 7 day cycle + possibly a monthly cycle
la.s7 <- artrans.wge(la$positivity, c(rep(0,6),1))

# Add (1-B) + s7
la.d1.s7 <- artrans.wge(la.s7,1)

# Try to model weekly + monthly data
la.s7.s12 <- artrans.wge(la$positivity, c(rep(0,6),1,0,0,0,0,1))

# Overfit table
est.ar.wge(la$positivity, p=15)

# Generate a factor table for a (1-B^7)
tswge::factor.wge(c(rep(0,6),1))

```

#### Estimate parameters and check residuals:

TSWGE offers an aic5 function that iterates through values of P (phi) and Q (theta), fits a model, and returns the top 5 performing models by either AIC or BIC value.  The AIC and BIC penalize models for the number of parameters used, with BIC favoring simpler models even more so than AIC.  This gives us a pretty good idea of the best fitting model as a starting point.

The ACF and PACF plots look like white noise, as expected.  The Ljung-Box test fails to reject the null hyphothesis that the residuals are white noise.  The generated spectral densities look like they have a fairly good fit, the final peak is overpronounced in our generations compared to the actual data.  The ACF comparisons are all over the board, but with peaks in the same general spots (lags at 7, 14, 21).  The generated realizations look quite a bit different, but the spectral densities and ACF plots look fairly similar, indicating this could be a useful model.

```{r Louisiana_estimate_weekly}
# Estimate the model params
aic5.wge(la.s7, type = 'bic', p=0:20,q=0:2) #15
e <- est.arma.wge(la.s7, p=15, q=0)

# Ljung Box Test shows white noise residuals
ljung.wge(artrans.wge(la.s7, phi.tr = e$phi))
ljung.wge(artrans.wge(la.s7, phi.tr = e$phi), K = 48)
acf(e$res)
pacf(e$res)

#Compare Spectral Densities
sims = 5
SpecDen = parzen.wge(la$positivity, plot = "FALSE")
plot(SpecDen$freq, SpecDen$pzgram, type = "l", lwd = 6)

for( i in 1: sims)
{
   SpecDen2 = parzen.wge(gen.aruma.wge(246,s = 7, phi = e$phi, plot ="FALSE"), plot = "FALSE")
   lines(SpecDen2$freq,SpecDen2$pzgram, lwd = 2, col = "red")
}


#Compare ACFs
sims = 5
ACF = acf(la$positivity, plot = "FALSE")
plot(ACF$lag ,ACF$acf , type = "l", lwd = 6)

for( i in 1: sims)
{
   ACF2 = acf(gen.aruma.wge(246, s = 7, phi = e$phi, plot = "FALSE"), plot = "FALSE")
   lines(ACF2$lag ,ACF2$acf, lwd = 2, col = "red")
}

#Compare Generated Realizations 
eGen = gen.aruma.wge(246, s = 7, phi = e$phi, vara = e$avar)
plotts.sample.wge(eGen)
plotts.sample.wge(la$positivity)


# Check performance
preds <- fore.aruma.wge(la$positivity, phi = e$phi, theta = e$theta, s=7, n.ahead = 7, lastn = T, limits = F)
eval_model(la$positivity,preds$f, model_name = 'AR(15) With Weekly/Monthly Trend', e$aic) #ASE .000647

preds <- fore.aruma.wge(la$positivity, phi = e$phi, theta = e$theta, s=7, n.ahead = 90, lastn = T, limits = F)
eval_model(la$positivity,preds$f, model_name = 'AR(15) With Weekly/Monthly Trend', e$aic) #ASE .000202

rolling <- rolling_ASE(la, e, s=7, horizon=7, model_name = 'AR(15) with Weekly Trend') #ASE .000277

```

#### Fitting additional models:

##### Weekly ARUMA(14,1,0)
ARUMA(14,1,0) with a weekly trend shows a slight improvement in the 7 day prediction, but a very poor performance in the 90 day trend.  This is due to the 1-B term induced by the difference, that causes the trend to continue in a downward trend seen in the training data.

```{r Louisiana_estimate_weekly_difference}
aic5.wge(la.d1.s7, p=0:15, q=0:2, type='bic') #BIC recommends 14,0
e <- est.arma.wge(la.d1.s7,p = 14, q=0)

preds <- fore.aruma.wge(la$positivity, phi = e$phi, theta=e$theta, d=1, s=7, n.ahead = 7, lastn = T, limits = F)
eval_model(la$positivity,preds$f, preds$ul, preds$ll,
           'ARIMA(14,1,0) With Weekly Trend', e$aic) #ASE = .000541

preds <- fore.aruma.wge(la$positivity, phi = e$phi, theta=e$theta, d=1, s=7, n.ahead = 90, lastn = T, limits = F)
eval_model(la$positivity,preds$f, preds$ul, preds$ll,
           'ARIMA(14,1,0) With Weekly Trend', e$aic) #ASE = .014017  (BAD)

rolling <- rolling_ASE(la, e, s=7, horizon=7, model_name = 'ARIMA(14,1,0) with Weekly Trend') #ASE .000679

```

##### Signal Plus Noise
The signal plus noise model performs very well in the 7 day window, but very poorly over the 90 day window. This is a deterministic signal with a stationary mean model, and our results aren't surprising since it doesn't account for the weekly trend well. 

```{r Louisiana_SigPlusNoise}
########## Sig Plus Noise ########## 
preds <- fore.sigplusnoise.wge(la$positivity, max.p = 15, n.ahead = 7, limits=F)
eval_model(la$positivity,preds$f, preds$ul, preds$ll,'SigPlusNoise', 0) #ASE = .000385

preds <- fore.sigplusnoise.wge(la$positivity, max.p = 15, n.ahead = 90, limits=F)
eval_model(la$positivity,preds$f, preds$ul, preds$ll,'SigPlusNoise', 0) #ASE = .003437 (BAD)

rolling <- rolling_ASE(la, e, s=7, horizon=7, model_name = 'SigPlusNoise', model_type = 'SigPlusNoise', maxP = 15) #ASE .000418

```

##### Neural Network
The neural network model performs fairly well, and picks up on the weekly trend, but regresses towards a mean, and is edged out by the AR(15) model from earlier.
```{r Louisiana_NeuralNetwork}
ts_la <- ts(la$positivity[1:239], start = '1')
x = mlp(ts_la, lags = 7, hd.auto.type = 'cv', reps=10)
plot(x)
preds <- predict(x, 7)
plot(preds)
eval_model(la$positivity,preds$mean,7,model_name = 'NNFOR', AIC_val = 0) #ASE = .000766


ts_la <- ts(la$positivity[1:156], start = '1')
x = mlp(ts_la, lags = 7, m = 1, hd.auto.type = 'cv', reps=100)
plot(x)
preds <- predict(x, 90)
plot(preds)
eval_model(la$positivity,preds$mean,90,model_name = 'NNFOR', AIC_val = 0) #ASE = .000852

rolling <- rolling_ASE(la, e, s=7, horizon=7, model_name = 'NNFOR', model_type = 'NNFOR') #ASE .000519

```



## United States

### Plot the Data:

```{r US_plots}
# Plot the data
plotts.sample.wge(us$positivity)
# Look deeper at spectral density
parzen.wge(us$positivity, trun=100)
```

There is surpsingly little evidence of serial correlation in the US data compared to Louisiana. There are very weak peaks in the spectral density plots and a very slowly damping behavior in the autocorrelation function plots.  Increasing the truncation point of the Parzen function doesn't show significant points to indicate a significant serial correlation.  

### Evaluate Stationarity:
There are few indications that suggest the data is not stationary, it appears that this could be a stationary time series with some wandering behavior. 

### Model Construction:
Differencing the data once shows no significant cyclical pattern in the ACF.

#### Identify correlation structures:
Here we are going to look at different filters to try to model out some of the correlation in the data.  We'll also look at a factor table to see if we can identify a weekly or monthly trend.

```{r US_correlation}

# Difference the data once
us.d1 <- artrans.wge(us$positivity,1)

# Difference it again, no improvement
artrans.wge(us.d1,1)

# Try various seasonal models
us.s7 <- artrans.wge(us$positivity, c(rep(0,6),1))
us.s12 <- artrans.wge(us$positivity, c(rep(0,11),1))
us.d1.s7 <- artrans.wge(us.d1, c(rep(0,6),1))
us.d1.s12 <- artrans.wge(us.d1, c(rep(0,11),1))

# Overfit table
est.ar.wge(us$positivity, p=15)

# Generate a factor table for a (1-B^7)
tswge::factor.wge(c(rep(0,6),1))

```

#### Estimate parameters and check residuals:

The ACF and PACF plots look like white noise, as expected.  The Ljung-Box test fails to reject the null hyphothesis that the residuals are white noise.  The generated spectral densities look like they have a fairly good fit, the final peak is overpronounced in our generations compared to the actual data.  The ACF comparisons are all over the board, but with peaks in the same general spots (lags at 7, 14, 21).  The generated realizations look quite a bit different, but the spectral densities and ACF plots look fairly similar, indicating this could be a useful model.

```{r US_estimate}
# Estimate the model params
aic5.wge(us$positivity, p=0:20, q=0:2, type='bic') # aic = 19,1 > 18,0 : bic = 1,1 > 9,1 > 2,1
e <- est.arma.wge(us$positivity, p=9, q=1)

# Ljung Box Test still shows white noise residuals
ljung.wge(e$res)
ljung.wge(e$res, K = 48)
acf(e$res)
pacf(e$res)

#Compare Spectral Densities
sims = 5
SpecDen = parzen.wge(us$positivity, plot = "FALSE")
plot(SpecDen$freq, SpecDen$pzgram, type = "l", lwd = 6)

for( i in 1: sims)
{
   SpecDen2 = parzen.wge(gen.aruma.wge(246,s = 0, phi = e$phi, plot ="FALSE"), plot = "FALSE")
   lines(SpecDen2$freq,SpecDen2$pzgram, lwd = 2, col = "red")
}

#Compare ACFs
sims = 5
ACF = acf(us$positivity, plot = "FALSE")
plot(ACF$lag ,ACF$acf , type = "l", lwd = 6)

for( i in 1: sims)
{
   ACF2 = acf(gen.aruma.wge(246, s = 0, phi = e$phi, plot = "FALSE"), plot = "FALSE")
   lines(ACF2$lag ,ACF2$acf, lwd = 2, col = "red")
}

#Compare Generated Realizations 
eGen = gen.aruma.wge(246, s = 0, phi = e$phi, vara = e$avar)
plotts.sample.wge(eGen)
plotts.sample.wge(us$positivity)

# Check performance
preds <- fore.aruma.wge(us$positivity, phi = e$phi, theta = e$theta, n.ahead = 7, lastn = T, limits = F)
eval_model(us$positivity,preds$f, model_name = 'AR(15) With Weekly/Monthly Trend', e$aic) #ASE .000121

preds <- fore.aruma.wge(us$positivity, phi = e$phi, theta = e$theta, n.ahead = 90, lastn = T, limits = F)
eval_model(us$positivity,preds$f, model_name = 'AR(15) With Weekly/Monthly Trend', e$aic) #ASE .00115

rolling <- rolling_ASE(us, e, s=0, horizon=7, model_name = 'AR(9,1)') #ASE .000096


```

#### Fitting additional models:

##### ARMA(18,1) with a weekly trend
The ARMA(18,1) with a weekly trend shows a slight improvement in the 7 day prediction, but a very poor performance in the 90 day trend.  This is due to the 1-B term induced by the difference, that causes the trend to continue in a downward trend seen in the training data.

```{r US_estimate_weekly_difference}

aic5.wge(us.s7, p=0:20, q=0:5, type='bic') #aic = 20,1 > 20,2 > 18,1 : bic = 20,1 > 18,1
e <- est.arma.wge(us.s7, p=18, q=1)

# Ljung Box Test still shows white noise residuals
ljung.wge(artrans.wge(us.s7, phi.tr = e$phi))
ljung.wge(artrans.wge(us.s7, phi.tr = e$phi), K = 48)
acf(e$res)
pacf(e$res)


#Compare Spectral Densities
sims = 5
SpecDen = parzen.wge(us$positivity, plot = "FALSE")
plot(SpecDen$freq, SpecDen$pzgram, type = "l", lwd = 6)

for( i in 1: sims)
{
   SpecDen2 = parzen.wge(gen.aruma.wge(246,s = 7, phi = e$phi, plot ="FALSE"), plot = "FALSE")
   lines(SpecDen2$freq,SpecDen2$pzgram, lwd = 2, col = "red")
}

#Compare ACFs
sims = 5
ACF = acf(us$positivity, plot = "FALSE")
plot(ACF$lag ,ACF$acf , type = "l", lwd = 6)

for( i in 1: sims)
{
   ACF2 = acf(gen.aruma.wge(246, s = 7, phi = e$phi, plot = "FALSE"), plot = "FALSE")
   lines(ACF2$lag ,ACF2$acf, lwd = 2, col = "red")
}

#Compare Generated Realizations 
eGen = gen.aruma.wge(246, s = 7, phi = e$phi, vara = e$avar)
plotts.sample.wge(eGen)
plotts.sample.wge(us$positivity)

# Check performance
preds <- fore.aruma.wge(us$positivity, phi = e$phi, theta = e$theta, s=7, n.ahead = 7, lastn = T, limits = F)
eval_model(us$positivity,preds$f, model_name = 'AR(15) With Weekly/Monthly Trend', e$aic) #ASE .000178

preds <- fore.aruma.wge(us$positivity, phi = e$phi, theta = e$theta, s=7, n.ahead = 90, lastn = T, limits = F)
eval_model(us$positivity,preds$f, model_name = 'AR(15) With Weekly/Monthly Trend', e$aic) #ASE .000707

rolling <- rolling_ASE(us, e, s=7, horizon=7, model_name = 'AR(15) with Weekly Trend') #ASE .000093

```

##### Signal Plus Noise
The signal plus noise model performs very well in the 7 day window, but very poorly over the 90 day window. This is a deterministic signal with a stationary mean model, and our results aren't surprising since it doesn't account for the weekly trend well. 

```{r US_SigPlusNoise}

preds <- fore.sigplusnoise.wge(us$positivity, max.p = 12, n.ahead = 7, limits=F)
eval_model(us$positivity,preds$f, preds$ul, preds$ll,'SigPlusNoise', 0) #ASE = .000169

preds <- fore.sigplusnoise.wge(us$positivity, max.p = 12, n.ahead = 90, limits=F)
eval_model(us$positivity,preds$f, preds$ul, preds$ll,'SigPlusNoise', 0) #ASE = .002678

rolling <- rolling_ASE(us, e, s=7, horizon=7, model_name = 'SigPlusNoise', model_type = 'SigPlusNoise', maxP = 15) #ASE .00012

```

##### Neural Network
The neural network model performs fairly well in the short term, and has the best rolling window ASE out of the bunch using minimal parameter tuning (setting lags = 7, and doing 5 fold cross validation).

```{r US_NeuralNetwork}

ts_us <- ts(us$positivity[1:239], start = '1')
x = mlp(ts_us, lags = 7, hd.auto.type = 'cv', reps=100)
plot(x)
preds <- predict(x, 7)
plot(preds)
eval_model(us$positivity,preds$mean, model_name = 'NNFOR', AIC_val =  0) #ASE = .00021


ts_us <- ts(us$positivity[1:156], start = '1')
x = mlp(ts_us, hd.auto.type = 'cv', reps=100, sel.lag = T)
plot(x)
preds <- predict(x, 90)
plot(preds)
eval_model(us$positivity,preds$mean,model_name = 'NNFOR', AIC_val =  0) #ASE = .002771

rolling <- rolling_ASE(us, e, s=0, horizon=7, model_name = 'NNFOR', model_type = 'NNFOR') #ASE .00009

```

# Multivariate Analysis 
Taking what we learned from our univariate analysis, let's see if we can add some explanatory variables to explain more difference in the data.  For Louisiana, I have selected average temperature, wind speed, precipitation, as well as significant events that I think would result in increased gatherings.  These include holidays, holiday weekends, the two weeks of school start dates across Louisiana, protests resulting from the Black Lives Matter movement, and various voting days.

For the US dataset, I will only be using the significant dates data, as the weather data only makes sense when applied to a specific region, such as Louisiana.

## Louisiana

```{r multivariate_Louisiana}

la_XDF <- data.frame(
  temp = ts(xdf$AvgTempF),
  humidity = ts(xdf$AvgHumidityPercent),
  wind = ts(xdf$AvgWindSpeedMPH),
  precip <- ts(xdf$PrecipitationIN),
  gathering <- ts(xdf$GatheringBinary)
)

x = mlp(ts_la, hd.auto.type = 'cv', reps=100, sel.lag = T, xreg = la_XDF)
plot(x)

## 7 day forecast
preds <- forecast(x, h=7, xreg=la_XDF)
plot(preds)
eval_model(la$positivity, preds$mean,preds$mean,preds$mean, model_name = 'NNFOR', AIC_val = 0) #ASE = .00085

## 90 day forecast
preds <- forecast(x, h=90, xreg=la_XDF)
plot(preds)
eval_model(la$positivity, preds$mean,preds$mean,preds$mean, model_name = 'NNFOR', AIC_val = 0) #ASE = .00085


```

```{r Louisiana_VAR}

VARselect(la$positivity, lag.max = 7, type = "const", season = 7, exogen = NULL) #AIC = 5.04
#VARselect picks p=2 using AIC and p=1 using BIC
vfit=VAR(la$positivity,p=2,type='const')
preds=predict(vfit,n.ahead=7)
preds

```

## United States

```{r multivariate_US}

us_XDF <- data.frame(
  gathering <- ts(xdf$GatheringBinary)
)

ts_us <- ts(us$positivity[1:239], start = '1')

x = mlp(ts_us, lags = 7, hd.auto.type = 'cv', reps=100, xreg = us_XDF)
plot(x)
preds <- predict(x, 7)
plot(preds)
eval_model(us$positivity,preds$mean,model_name='NNFOR', AIC_val =  0) #ASE = 


ts_us <- ts(us$positivity[1:156], start = '1')
x = mlp(ts_us, hd.auto.type = 'cv', reps=100, lags =  7, xreg = us_XDF)
plot(x)
preds <- predict(x, 90)
plot(preds)
eval_model(us$positivity,preds$mean, model_name='NNFOR', AIC_val =  0) #ASE = 


```

# Model Comparison



# Summary